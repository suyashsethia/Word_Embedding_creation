{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data from sentences\n",
    "with open('sentences.json') as f:\n",
    "    sentences = json.load(f)\n",
    "\n",
    "# sentences = sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, min_count=5):\n",
    "    freq_dist =[]\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "    word_counts['<UNK>']=0\n",
    "    k = 0 \n",
    "    #words which occur less than min_count are replaced with <UNK> token\n",
    "    for word, count in word_counts.items():\n",
    "        if count < min_count:\n",
    "            word_counts['<UNK>'] += count\n",
    "            k+=count\n",
    "        else:\n",
    "            freq_dist.append(count)\n",
    "    \n",
    "    freq_dist.append(k)\n",
    "            \n",
    "\n",
    "    #add <UNK> to vocab \n",
    "    vocab = [word for word, count in word_counts.items() if count >= min_count]\n",
    "    vocab.append(\"UNK\")\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(vocab)}\n",
    "    return vocab, word2idx, idx2word , freq_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'great', 'tips', 'always', 'helping', 'complete', 'Good', 'collection', '', 'I']\n"
     ]
    }
   ],
   "source": [
    "vocab, word2idx, idx2word ,freq_dist = build_vocab(sentences)\n",
    "print(vocab[:10])\n",
    "# word2idx[\"UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace words in sentences having count less than min_count with <UNK> token\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        if word not in vocab:\n",
    "            sentences[i][j] = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(sentences, word2idx):\n",
    "    dataset = []\n",
    "    for sentence in sentences:\n",
    "        for i in range(window_size, len(sentence) - window_size):\n",
    "            # context = sentence[i - window_size: i] + sentence[i + 1: i + window_size + 1]\n",
    "            target = sentence[i]\n",
    "            target_index = word2idx[target]\n",
    "            context_indices = []\n",
    "            for j in range(i - window_size, i + window_size + 1):\n",
    "                if j != i:\n",
    "                    context_indices.append(word2idx[sentence[j]])     \n",
    "            dataset.append((context_indices, target_index))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= build_dataset(sentences, word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_SAMPLE_SIZE =30\n",
    "def negative_sampling(dataset, word2idx, vocab_size, k=5):\n",
    "    normalized_freq = F.normalize(\n",
    "    torch.Tensor(freq_dist).pow(0.75), dim=0)  # p(w)^0.75\n",
    "    weights = torch.ones(len(freq_dist))  # weights for each word\n",
    "    for _ in tqdm(range(len(freq_dist))):\n",
    "        for _ in range(NEG_SAMPLE_SIZE):\n",
    "            neg_index = torch.multinomial(normalized_freq, 1)[\n",
    "            0]  # sample a word\n",
    "        # increase the weight of the sampled word\n",
    "            weights[neg_index] += 1\n",
    "\n",
    "    return weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27998/27998 [06:51<00:00, 68.07it/s]\n"
     ]
    }
   ],
   "source": [
    "weights = negative_sampling(dataset, word2idx, len(vocab), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_embeds = self.embeddings(inputs)\n",
    "        embeds = torch.mean(input_embeds, dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch._six import with_metaclass\n",
    "\n",
    "\n",
    "class VariableMeta(type):\n",
    "    def __instancecheck__(cls, other):\n",
    "        return isinstance(other, torch.Tensor)\n",
    "\n",
    "# mypy doesn't understand torch._six.with_metaclass\n",
    "class Variable(with_metaclass(VariableMeta, torch._C._LegacyVariableBase)):  # type: ignore[misc]\n",
    "    pass\n",
    "\n",
    "from torch._C import _ImperativeEngine as ImperativeEngine\n",
    "Variable._execution_engine = ImperativeEngine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE =0.001\n",
    "EMBEDDING_SIZE = 30\n",
    "NUM_REVIEWS = 100\n",
    "model = CBOW(len(vocab), EMBEDDING_SIZE)\n",
    "def train(num_epochs):\n",
    "        losses = []\n",
    "        loss_fn = nn.NLLLoss(weight=weights)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch+1}\")\n",
    "            total_loss = 0\n",
    "            for i in tqdm(range(0, len(dataset), BATCH_SIZE)):\n",
    "                batch = dataset[i: i + BATCH_SIZE]\n",
    "                \n",
    "\n",
    "                context = [x[0] for x in batch]\n",
    "                focus = [x[1] for x in batch]\n",
    "                \n",
    "\n",
    "                context_var = Variable(torch.LongTensor(context))\n",
    "                focus_var = Variable(torch.LongTensor(focus))\n",
    "                \n",
    "                optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "                optimizer.zero_grad()\n",
    "                log_probs = model(context_var)\n",
    "                loss = loss_fn(log_probs, focus_var)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            losses.append(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1408/63232 [01:20<59:09, 17.42it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[236], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(\u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[235], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m log_probs \u001b[39m=\u001b[39m model(context_var)\n\u001b[1;32m     27\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(log_probs, focus_var)\n\u001b[0;32m---> 28\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding( word_idx):\n",
    "    embedding_index = Variable(torch.LongTensor([word_idx]))\n",
    "    return model.embeddings(embedding_index).data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embedding(word2idx[\"UNK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(_word, k):\n",
    "    word = _word.lower()\n",
    "    if word not in vocab:\n",
    "        print(\n",
    "            f\"{_word} not in vocabulary. Falling back to Out-of-Vocabulary Token\")\n",
    "        word = \"UNK\"\n",
    "    distances = []\n",
    "    focus_index = word2idx[word]\n",
    "    focus_embedding = get_embedding(focus_index)\n",
    "    for i in range(1, len(vocab)):\n",
    "        if i == focus_index:\n",
    "            continue\n",
    "        comp_embedding = get_embedding(i)\n",
    "        comp_word = idx2word[i]\n",
    "        dist = cosine(focus_embedding, comp_embedding)\n",
    "        distances.append({'Word': comp_word, 'Distance': dist})\n",
    "    distances = sorted(distances, key=lambda x: x['Distance'])\n",
    "    return pd.DataFrame(distances[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the embeddings for all fords in vocab in file\n",
    "import pickle\n",
    "embeddings = []\n",
    "for i in range(1, len(vocab)):\n",
    "    embeddings.append(get_embedding(i))\n",
    "with open('data.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vectors from W2V_3_epochs.json\n",
    "with open('data.pkl', 'r') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3519095997.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    word2vecc.update(vocab[i]:get_embedding(vocab[i]))\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "word2vec = {}\n",
    "for i in range(1, len(vocab)):\n",
    "    word2vec[idx2word[i]] = embeddings[i-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
